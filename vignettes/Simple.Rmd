---
title: "A simple example of how to use voluModel for niche modeling"
author:
- Hannah L. Owens
- Carsten Rahbek
date: "`r Sys.Date()`"
output:
  rmarkdown::html_vignette:
    fig_caption: yes
    toc: true
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{A simple example of how to use voluModel for niche modeling}
  \usepackage[utf8]{inputenc}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
knitr::opts_knit$set(root.dir = system.file('extdata/', package='voluModel'))
load(system.file("extdata/smoothOxygen.RData", 
                             package='voluModel'))
```

# Introduction 
To date, distribution and ecological models of marine species have generally only considered distributions in two-dimensional space, not accounting for depth. This may lead to mis-estimation of species distributions and subsequent diversity estimates, especially among pelagic and benthic species. Here, we present a possible way forward: generating species distribution models based on environmental data extracted at the depths where individuals were observed, and calibrated with three-dimensional sampling of pseudo-absences. We also provide some methods to project these models back into three-dimensional space and visualize the results. 

# Data Inputs

First thing's first. Distributional models are based on coordinates of species' occurrences. Here is an example dataset of Aphanopus intermedius, Intermediate Scabbardfish, using data downloaded via R (R Core Team, 2020) from GBIF (Chamberlain et al., 2021; Chamberlain and Boettiger, 2017) and OBIS (Provoost and Bosch, 2019) via occCite (Owens et al., 2021).

```{r occurrence data, eval=T}
occs <- read.csv(system.file("extdata/Aphanopus_intermedius.csv", 
                             package='voluModel'))
head(occs)
```

Next, we load two environmental datasets from the World Ocean Atlas (Garcia et al., 2019): temperature (Locarnini et al., 2018) and apparent oxygen utilization (Garcia et al., 2019). We have chosen these variables for simplified illustrative purposes--we recommend you explore additional variables from the World Ocean Atlas and other sources. These data are supplied by the World Ocean Atlas as point shapefiles, so our first task is to read in the shapefiles and convert each variable into a `RasterBrick`. 

```{r environmental data loading temperature, eval=T}
library(rgdal)
library(raster)
library(voluModel)
temperature <- readOGR(system.file("extdata/woa18_decav_t00mn01_shape/woa18_decav_t00mn01_cropped.shp", 
                                   package = "voluModel"))
temperature@data[temperature@data == -999.999] <- NA
temperature <- rasterFromXYZ(cbind(temperature@coords,
                                   temperature@data))

# Get names of depths
envtNames <- gsub("[d,M]", "", names(temperature))
envtNames[[1]] <- "0"
names(temperature) <- envtNames

plot(temperature[[1]], 
     main = c("Temperature, surface"))
```

Apparent oxygen usage is more patchily sampled than temperature (it's generally measured from instrument casts on research cruises), so we use the `interpolateRaster()` function to produce statistically-interpolated layers using `TPS()` from the `fields` package. Be patient--this step can take a while, although as shown here, I am using the `fastTPS()` approximation. Maybe this will work ok for your data, maybe it won't. It depends on the data.

```{r environmental data loading oxygen, eval=T}
library(fields)
oxygen <- readOGR(system.file("extdata/woa18_all_A00mn01_shape/woa18_all_A00mn01_cropped.shp",
                              package = "voluModel"))
oxygen@data[oxygen@data == -999.999] <- NA
oxygen <- rasterFromXYZ(cbind(oxygen@coords, oxygen@data))

for (i in 1:nlayers(oxygen)){
  oxygen[[i]] <- interpolateRaster(oxygen[[i]], lon.lat = T, fast = T, aRange = 5) #Thin plate spline interpolation
  oxygen[[i]] <- crop(mask(x = oxygen[[i]], mask = temperature[[i]]), temperature[[i]])
}

# Change names to match temperature
names(oxygen) <- envtNames
rm(envtNames)

plot(oxygen[[1]], 
     main = "Apparent Oxygen Utilization, surface")
```

You can see from these plots that apparent oxygen utilization is still "patchier" than temperature. If you are confident that there should be a strong spatial correlation in a "patchy" data layer, you can also statistically smooth it, again using `TPS()`, like this:

```{r smoothing oxygen, eval=FALSE}
oxygenSmooth <- oxygen
for (i in 1:nlayers(oxygen)){
  oxygenSmooth[[i]] <- smoothRaster(oxygenSmooth[[i]], lon.lat = T) #Thin plate spline interpolation
  oxygenSmooth[[i]] <- crop(mask(x = oxygenSmooth[[i]], mask = temperature[[i]]), temperature[[i]])
}

# Change names to match temperature
names(oxygenSmooth) <- names(temperature)

plot(oxygenSmooth[[1]], 
     main = "Apparent Oxygen Utilization, surface")
```
```{r smooth oxygen display, echo=FALSE}
plot(oxygenSmooth[[1]], 
     main = "Smoothed Apparent Oxygen Utilization, surface")
```

# Sampling data for model generation

The first step is down-sampling the occurrence data so that there is only one occurrence per voxel (i.e. 3D pixel) of environmental data. We do this to avoid over-fitting the model due to biased sampling. The resampled points are centered in each voxel.

```{r downsample to voxel, eval=TRUE, warning=FALSE}
occurrences <- occs[,c("decimalLatitude", "decimalLongitude", "depth")] 

# Preliminary cleaning
occurrences <- dplyr::distinct(occurrences)
occurrences <- occurrences[complete.cases(occurrences),]

# Gets the layer index for each occurrence by matching to depth
layerNames <- as.numeric(gsub("[X]", "", names(temperature)))
occurrences$index <- unlist(lapply(occurrences$depth, FUN = function(x) which.min(abs(layerNames - x))))
indices <- unique(occurrences$index)
downsampledOccs <- data.frame()
for(i in indices){
  tempPoints <- occurrences[occurrences$index==i,]
  tempPoints <- downsample(tempPoints, temperature[[1]])
  tempPoints$depth <- rep(layerNames[[i]], times = nrow(tempPoints))
  downsampledOccs <- rbind(downsampledOccs, tempPoints)
}

occurrences <- downsampledOccs

print(paste0("Original number of points: ", nrow(occs), "; number of downsampled occs: ", nrow(occurrences)))
pointCompMap(occs1 = occs, occs2 = occurrences, 
             occs1Name = "Original", occs2Name = "Cleaned", 
             spName = "Aphanopus intermedius", 
             land = rnaturalearth::ne_countries(scale = "medium", returnclass = "sf")[1]
)
rm(indices, layerNames, tempPoints, i, downsampledOccs, occs)
```

Next, we generate a sampling region for the model based on occurrence points. Note that ideally when you're modeling you would carefully curate this background sampling region to make sure it truly approximates the area accessible to the species you are modeling. However, if you do not have a lot of information regarding accessible areas, the function supplied here may be useful to at least generate something logical and repeatable. `marineBackground` is a special case wrapper around `getDynamicAlphaHull` from the `rangeBuilder` package. `getDynamicAlphaHull` generates an alpha hull polygon around occurrence points; it iteratively adjusts the alpha parameter (how tightly the hull adheres to the points) to meet fit a hull around the data per the user's requirements. One can give it a buffer distance around points, as well as the minimum fraction of occurrences that must be found within the shapefile and the number of allowable distinct polygons. What `marineBackground` brings to the table is 1) robust functionality when the hull expands beyond 180 degrees East and/or West. Regions beyond this meridian are wrapped to the appropriate hemisphere instead of being deleted.

```{r environmental background sampling}
backgroundSamplingRegions <- marineBackground(occurrences)
plot(temperature[[1]], 
     main = "Points and background sampling plotted on surface temperature",
     col = viridisLite::mako(n= 11))
plot(backgroundSamplingRegions, add = T, border = "orange", lwd = 2)
points(occurrences[,c("decimalLongitude","decimalLatitude")], 
       cex = 2, pch = 20, col = "red")
```

Finally, we need to draw the environmental variable data from each occurrence voxel, as well as from the background sampling region. First, we sample data for occurrences. Then, we generate background data by drawing occurrences from the XY extent of the background shapefile and from user-specified depths from 100 to 1500m. Background points intersecting with occurrences are not returned. Finally, the environmental data at each background coordinate are drawn and stored in their own data object.

```{r sampling}
# Presences
oxyVals <- xyzSample(occs = occurrences, oxygenSmooth)
tempVals <- xyzSample(occs = occurrences, temperature)
vals <- cbind(occurrences, oxyVals, tempVals)
colnames(vals) <- c("decimalLongitude", "decimalLatitude", "depth", "AOU", "Temperature")
vals <- vals[complete.cases(vals),]
row.names(vals) <- NULL
occsWdata <- vals

# Background
backgroundVals <- mSampling3D(occs = occurrences, 
                              envBrick = temperature, 
                              mShp = backgroundSamplingRegions, 
                              depthLimit = c(50, 1500))
oxyVals <- xyzSample(occs = backgroundVals, oxygenSmooth)
tempVals <- xyzSample(occs = backgroundVals, temperature)
vals <- cbind(backgroundVals, oxyVals, tempVals)
colnames(vals) <- c("decimalLongitude", "decimalLatitude", "depth", "AOU", "Temperature")
vals <- vals[complete.cases(vals),]
row.names(vals) <- NULL
backgroundWdata <- vals
rm(oxyVals, tempVals, vals, backgroundVals)
```

# Niche envelope models

First, let's generate a very simple envelope model, in which we define suitable conditions as those within the maximum and minimum conditions at which *Aphanopus intermedius* has been observed.

```{r generate envelope niche model}
# Get limits
tempLims <- quantile(occsWdata$Temperature,c(0, 1))
aouLims <- quantile(occsWdata$AOU,c(0, 1))

# Reclassify environmental bricks to presence/absence
temperaturePresence <- reclassify(temperature, 
                                  rcl = c(-Inf,tempLims[[1]],0,
                                          tempLims[[1]], tempLims[[2]], 1,
                                          tempLims[[2]], Inf, 0))
AOUpresence <- reclassify(oxygenSmooth, 
                          rcl = c(-Inf, aouLims[[1]],0,
                                  aouLims[[1]], aouLims[[2]], 1,
                                  aouLims[[2]], Inf, 0))

# Put it all together
envelopeModel <- temperaturePresence * AOUpresence
envelopeModel <- mask(envelopeModel, backgroundSamplingRegions)
envelopeModel <- crop(envelopeModel, backgroundSamplingRegions)
names(envelopeModel) <- names(temperature)
rm(tempLims, aouLims, temperaturePresence, AOUpresence)
```

It's hard to visualize things in three dimensions, but here's an attempt. `plotLayers` plots a transparent layer of suitable habitat for each depth layer. The redder the color, the shallower the layer, the bluer, the deeper. The more saturated the color, the more layers with suitable habitat. Here, I am plotting suitability from 55m to 1300m, the depth range of occurrences used to train the envelope model.

```{r 3D envelope model}
# Get indices of model-relevant layers
layerNames <- as.numeric(gsub("[X]", "", names(envelopeModel)))
occurrences$index <- unlist(lapply(occurrences$depth, 
                                   FUN = function(x) 
                                     which.min(abs(layerNames - x))))
indices <- sort(unique(occurrences$index))

# Get continent shapefile
land <- rnaturalearth::ne_countries(scale = "medium", 
                                    returnclass = "sf")[1]

plotLayers(envelopeModel[[min(indices):max(indices)]], 
          land = land)
rm(layerNames, indices)
```

# Generalized linear models

Here's another example of a more complicated application, generating a general linear model. For this, we will need to unite the presence and absence data into a single data frame with an additional column, "response", which will contain 1s for presences and 0s for absences.

```{r glm data prep part 1}
# Add response as a column
occsWdata$response <- rep(1, times = nrow(occsWdata))
backgroundWdata$response <- rep(0, times = nrow(backgroundWdata))
```

If you are concerned about overfitting the model because you generated a lot more background points than presence points, you can sample from the background data however you think is logical. The example below samples twice the number of occurrence points from the background, weighted by distance form the suitable centroid of occurrence points. That is, the more environmentally-different a background point is from an occurrence point, the more likely it is to be sampled. This is helpful for GLMs, because background points are interpreted as "absences" unlike in methods like Maxent, where sampling the most dissimilar background is more likely to lead to an overfit model.

```{r glm data prep part 2}
# Sample background points weighted by distance from centroid of occurrence environments
suitableCentroid <- apply(occsWdata[,c("Temperature", "AOU")], 
                          MARGIN = 2, FUN = mean)
backgroundWdata$distance <- apply(backgroundWdata[,c("Temperature", "AOU")], MARGIN = 1, 
                                  FUN = function(x) dist(rbind(suitableCentroid, x)))
backgroundWdata$sampleWeight <- (backgroundWdata$distance - min(backgroundWdata$distance))/(max(backgroundWdata$distance)-min(backgroundWdata$distance))
sampleForAbsence <- sample(x = rownames(backgroundWdata), 
                           size = nrow(occsWdata) * 100, 
                           prob = backgroundWdata$sampleWeight)
backgroundWdata <- backgroundWdata[match(sampleForAbsence, rownames(backgroundWdata)),]

# Unite datasets
datForMod <- rbind(occsWdata, backgroundWdata[,colnames(occsWdata)])
rm(suitableCentroid, sampleForAbsence, backgroundWdata, occsWdata)
```

Now we are ready to make a generalized linear model. How does it look?

```{r generate glm niche model}
glmModel <- glm(formula = response ~ Temperature *  AOU, 
                  family = binomial(link = "logit"),  data = datForMod)
summary(glmModel)
```

Next we project the model back into geographic space.

```{r project glm niche model}
layerNames <- as.numeric(gsub("[X]", "", names(temperature)))
index <- seq(from = match(min(datForMod$depth), layerNames), 
             to = match(max(datForMod$depth), layerNames), by = 1)
depthPred <- NULL
for(j in index){
  depthPreds <- stack(temperature[[j]], oxygenSmooth[[j]])
  crs(depthPreds) <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
  names(depthPreds) <- c("Temperature", "AOU")
  depthPred[[j]] <- mask(predict(depthPreds, glmModel), backgroundSamplingRegions)
  depthPred[[j]] <- crop(depthPred[[j]], backgroundSamplingRegions)
  names(depthPred[[j]]) <- layerNames[[j]]
}
glmPred <- stack(depthPred[!unlist(lapply(depthPred, FUN = function(x) is.null(x)))])
rm(index, j, layerNames, depthPred, depthPreds, backgroundSamplingRegions, glmModel)
```

And here's what it looks like threshholded to presences and absences. In this case, the threshold is the bottom tenth percentile of suitability scores.

```{r glm niche threshold}
glmThreshold <- quantile(xyzSample(datForMod[datForMod$response == 1,
                                             c("depth", "decimalLongitude", "decimalLatitude")], 
                                   brick(glmPred)), .1, na.rm = T)[[1]] # MS90
glmThresholded <- glmPred > glmThreshold
glmThresholded <- reclassify(glmThresholded, c(NA, NA, 0), include.lowest = T)
rm(glmThreshold, glmPred)
```

And plotted like the envelope model.

```{r thresholded glm niche model plotted}
layerNames <- as.numeric(gsub("[X]", "", names(glmThresholded)))
datForMod$index <- unlist(lapply(datForMod$depth, FUN = function(x) which.min(abs(layerNames - x))))
indices <- unique(datForMod$index)

plotLayers(glmThresholded[[min(indices):max(indices)]], 
          land = land, landCol = "black")
```

Of course, these are just two starting examples. We welcome submissions from the community for other types of models you would like to see. Submit your suggestions as issues [here](https://github.com/hannahlowens/voluModel/issues).

# References

Catania D, Fong J (2020). CAS Ichthyology (ICH). Version 150.241. California Academy of Sciences. Occurrence dataset https://doi.org/10.15468/efh2ib accessed via GBIF.org on 2020-09-24.

Chic Giménez Ò, Lombarte Carrera A (2018). Colección de referencia de otolitos, Instituto de Ciencias del Mar-CSIC. Institute of Marine Sciences (ICM-CSIC). Occurrence dataset https://doi.org/10.15468/wdwxid accessed via GBIF.org on 2020-09-24.

Chamberlain S, Barve V, Mcglinn D, Oldoni D, Desmet P, Geffert L, Ram K (2021). rgbif: Interface to the Global Biodiversity Information Facility API. R package version 3.6.0, https://CRAN.R-project.org/package=rgbif.

Chamberlain S, Boettiger C (2017). “R Python, and Ruby clients for GBIF species occurrence data.” PeerJ PrePrints. https://doi.org/10.7287/peerj.preprints.3304v1.

Davis Rabosky AR, Cox CL, Rabosky DL, Title PO, Holmes IA, Feldman A, McGuire JA (2016). Coral snakes predict the evolution of mimicry across New World snakes. Nature Communications 7:11484.

European Nucleotide Archive (EMBL-EBI) (2019). Geographically tagged INSDC sequences. Occurrence dataset https://doi.org/10.15468/cndomv accessed via GBIF.org on 2020-09-24.

Frable B (2019). SIO Marine Vertebrate Collection. Version 1.7. Scripps Institution of Oceanography. Occurrence dataset https://doi.org/10.15468/ad1ovc accessed via GBIF.org on 2020-09-24.

Garcia HE, Boyer TP, Baranova OK, Locarnini RA, Mishonov AV, Grodsky A, Paver CR, Weathers KW, Smolyar IV, Reagan JR, Seidov D, Zweng MM (2019). World Ocean Atlas 2018: Product Documentation. A Mishonov, Technical Editor.

Garcia HE, Weathers K, Paver CR, Smolyar I, Boyer TP, Locarnini RA, Zweng MM, Mishonov AV, Baranova OK, Seidov D, Reagan JR (2018). World Ocean Atlas 2018, Volume 3: Dissolved Oxygen, Apparent Oxygen Utilization, and Oxygen Saturation. A Mishonov Technical Ed.; NOAA Atlas NESDIS 83, 38pp.

GBIF.org (24 September 2020) GBIF Occurrence Download https://doi.org/10.15468/dl.efuutj.

Grant S, McMahan C (2020). Field Museum of Natural History (Zoology) Fish Collection. Version 13.12. Field Museum. Occurrence dataset https://doi.org/10.15468/alz7wu accessed via GBIF.org on 2020-09-24.

Locarnini RA, Mishonov AV, Baranova OK, Boyer TP, Zweng MM, Garcia HE, Reagan JR, Seidov D, Weathers K, Paver CR, Smolyar I (2018). World Ocean Atlas 2018, Volume 1: Temperature. A. Mishonov Technical Ed.; NOAA Atlas NESDIS 81, 52pp.

Menezes G (2020). Demersais survey in the Azores between 1996 and 2013. Version 1.1. Institute of Marine Research. Occurrence dataset https://doi.org/10.14284/22 accessed via GBIF.org on 2020-09-24.

Natural History Museum (2020). Natural History Museum (London) Collection Specimens. Occurrence dataset https://doi.org/10.5519/0002965 accessed via GBIF.org on 2020-09-24.

Norén M, Shah M (2017). Fishbase. FishBase. Occurrence dataset https://doi.org/10.15468/wk3zk7 accessed via GBIF.org on 2020-09-24.

Nychka D, Furrer R, Paige J, Sain S (2021). “fields: Tools for spatial data.” R package version 13.3, <URL:
https://github.com/dnychka/fieldsRPackage>.

Orrell T (2020). NMNH Extant Specimen Records. Version 1.35. National Museum of Natural History, Smithsonian Institution. Occurrence dataset https://doi.org/10.15468/hnhrg3 accessed via GBIF.org on 2020-09-24.

Owens H, Merow C, Maitner B, Kass J, Barve V, Guralnick R (2021). occCite: Querying and Managing Large Biodiversity Occurrence Datasets_. doi: 10.5281/zenodo.4726676 (URL: https://doi.org/10.5281/zenodo.4726676), R package version 0.4.9.9000, (<URL: https://CRAN.R-project.org/package=occCite>).

Provoost P, Bosch S (2019). “robis: R Client to access data from the OBIS API.” Ocean Biogeographic Information System. Intergovernmental Oceanographic Commission of UNESCO. R package version 2.1.8, https://cran.r-project.org/package=robis.

R Core Team (2017). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. URL https://www.R-project.org/.
